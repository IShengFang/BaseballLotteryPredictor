{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import json\n",
    "date = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06',\n",
    "        'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}\n",
    "week = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetData(year, data_time):\n",
    "    url = 'http://www.baseball-reference.com/previews/' + year + '/' + data_time + '.shtml'\n",
    "    data = pd.read_html(url)\n",
    "    x = data[0]\n",
    "    x.columns = range(x.shape[1])\n",
    "    x = x[x[0].notnull()]\n",
    "    x = x.loc[list(map(lambda x: len(x) > 50, x[0]))]\n",
    "    x = x.loc[:, 0:1]\n",
    "\n",
    "    ## road\n",
    "    texts = x[0][1]\n",
    "    texts = texts.replace('(', ' (')\n",
    "    texts = texts.replace(':', ' ')\n",
    "\n",
    "    # team name\n",
    "    team_name = \"\"\n",
    "    for text in texts:\n",
    "        if text.isdigit():\n",
    "            break\n",
    "        team_name += text\n",
    "\n",
    "    # data detail\n",
    "    texts_split = texts.split(\" \")\n",
    "    while \"\" in texts_split:\n",
    "        texts_split.remove(\"\")\n",
    "    texts_detail = texts_split[texts_split.index(\"Home\"):texts_split.index(\"IL\")+2]\n",
    "    road = dict({\"Team name\": team_name})\n",
    "    for index in range(0, len(texts_detail), 2):\n",
    "        road[texts_detail[index]] = texts_detail[index+1]\n",
    "\n",
    "    # last 10 games\n",
    "    last_10_games = texts_split.copy()\n",
    "    last_10_games.remove('Last')\n",
    "    last_10_games = last_10_games[last_10_games.index('Place/GB')+1:last_10_games.index('Last')]\n",
    "    while \"\" in last_10_games:\n",
    "        last_10_games.remove(\"\")\n",
    "    for item in last_10_games:\n",
    "        if \"(\" in item:\n",
    "            last_10_games.remove(item)\n",
    "    while 'gb' in last_10_games:\n",
    "        last_10_games.remove('gb')\n",
    "    while 'up' in last_10_games:\n",
    "        last_10_games.remove('up')\n",
    "    while '-' in last_10_games:\n",
    "        if last_10_games.index('-')%10 == 5:\n",
    "            last_10_games[last_10_games.index('-')] = 'T'\n",
    "        else:\n",
    "            fore = last_10_games[:last_10_games.index('-')]\n",
    "            back = last_10_games[last_10_games.index('-')+1:]\n",
    "            last_10_games = fore + [None, None, None] + back\n",
    "    last_10_game_list = []\n",
    "    for i in range(0, 100, 10):\n",
    "        if i > len(last_10_games)-1:\n",
    "            last_10_game_list.append([None, None, None, None, None, None, None, None])\n",
    "        else:\n",
    "            if len(last_10_games[i+3]) < 2:\n",
    "                day = date[last_10_games[i+2]] + '0' + last_10_games[i+3]\n",
    "            else:\n",
    "                day = date[last_10_games[i+2]] + last_10_games[i+3]\n",
    "            if '@' in last_10_games[i+4]:\n",
    "                opp = last_10_games[i+4]\n",
    "                opp = opp.replace(\"@\", \"\")\n",
    "                flag = 0\n",
    "            else:\n",
    "                opp = last_10_games[i+4]\n",
    "                flag = 1\n",
    "            if last_10_games[i+5] == 'W':\n",
    "                W_L = 1\n",
    "            elif last_10_games[i+5] == 'L':\n",
    "                W_L = 0\n",
    "            else:\n",
    "                W_L = 0.5\n",
    "            if len(last_10_games[i+3]) == 1:\n",
    "                last_10_games[i+3] = '0' + last_10_games[i+3]\n",
    "            if last_10_games[i+7] != None:\n",
    "                record = last_10_games[i+7]\n",
    "            else:\n",
    "                record = None\n",
    "            if last_10_games[i+8] != None:\n",
    "                place = int(last_10_games[i+8][0])\n",
    "            else:\n",
    "                place = None\n",
    "            if last_10_games[i+9] != None:\n",
    "                try:\n",
    "                    GB = float(last_10_games[i+9])\n",
    "                except:\n",
    "                    GB = 0.0\n",
    "            else:\n",
    "                GB = None\n",
    "            last_10_game_list.append([day,\n",
    "                                      flag,\n",
    "                                      opp,\n",
    "                                      W_L,\n",
    "                                      last_10_games[i+6],\n",
    "                                      record,\n",
    "                                      place,\n",
    "                                      GB])\n",
    "    road[\"last_10_game\"] = last_10_game_list\n",
    "\n",
    "\n",
    "    ## home\n",
    "    texts = x[1][1]\n",
    "    texts = texts.replace('(', ' (')\n",
    "    texts = texts.replace(':', ' ')\n",
    "\n",
    "    # team name\n",
    "    team_name = \"\"\n",
    "    for text in texts:\n",
    "        if text.isdigit():\n",
    "            break\n",
    "        team_name += text\n",
    "\n",
    "    # data detail\n",
    "    texts_split = texts.split(\" \")\n",
    "    while \"\" in texts_split:\n",
    "        texts_split.remove(\"\")\n",
    "    texts_detail = texts_split[texts_split.index(\"Home\"):texts_split.index(\"IL\")+2]\n",
    "    home = dict({\"Team name\": team_name})\n",
    "    for index in range(0, len(texts_detail), 2):\n",
    "        home[texts_detail[index]] = texts_detail[index+1]\n",
    "\n",
    "    # last 10 games\n",
    "    last_10_games = texts_split.copy()\n",
    "    last_10_games.remove('Last')\n",
    "    last_10_games = last_10_games[last_10_games.index('Place/GB')+1:last_10_games.index('Last')]\n",
    "    while \"\" in last_10_games:\n",
    "        last_10_games.remove(\"\")\n",
    "    for item in last_10_games:\n",
    "        if \"(\" in item:\n",
    "            last_10_games.remove(item)\n",
    "    while 'gb' in last_10_games:\n",
    "        last_10_games.remove('gb')\n",
    "    while 'up' in last_10_games:\n",
    "        last_10_games.remove('up')\n",
    "    while '-' in last_10_games:\n",
    "        if last_10_games.index('-')%10 == 5:\n",
    "            last_10_games[last_10_games.index('-')] = 'T'\n",
    "        else:\n",
    "            fore = last_10_games[:last_10_games.index('-')]\n",
    "            back = last_10_games[last_10_games.index('-')+1:]\n",
    "            last_10_games = fore + [None, None, None] + back\n",
    "            last_10_games = fore + [None, None, None] + back+ [None, None, None] + back\n",
    "    last_10_game_list = []\n",
    "    for i in range(0, 100, 10):\n",
    "        if i > len(last_10_games)-1:\n",
    "            last_10_game_list.append([None, None, None, None, None, None, None, None])\n",
    "        else:\n",
    "            if len(last_10_games[i+3]) < 2:\n",
    "                day = date[last_10_games[i+2]] + '0' + last_10_games[i+3]\n",
    "            else:\n",
    "                day = date[last_10_games[i+2]] + last_10_games[i+3]\n",
    "            if '@' in last_10_games[i+4]:\n",
    "                opp = last_10_games[i+4]\n",
    "                opp = opp.replace(\"@\", \"\")\n",
    "                flag = 0\n",
    "            else:\n",
    "                opp = last_10_games[i+4]\n",
    "                flag = 1\n",
    "            if last_10_games[i+5] == 'W':\n",
    "                W_L = 1\n",
    "            elif last_10_games[i+5] == 'L':\n",
    "                W_L = 0\n",
    "            else:\n",
    "                W_L = 0.5\n",
    "            if len(last_10_games[i+3]) == 1:\n",
    "                last_10_games[i+3] = '0' + last_10_games[i+3]\n",
    "            if last_10_games[i+7] != None:\n",
    "                record = last_10_games[i+7]\n",
    "            else:\n",
    "                record = None\n",
    "            if last_10_games[i+8] != None:\n",
    "                place = int(last_10_games[i+8][0])\n",
    "            else:\n",
    "                place = None\n",
    "            if last_10_games[i+9] != None:\n",
    "                try:\n",
    "                    GB = float(last_10_games[i+9])\n",
    "                except:\n",
    "                    GB = 0.0\n",
    "            else:\n",
    "                GB = None\n",
    "            last_10_game_list.append([day,\n",
    "                                      flag,\n",
    "                                      opp,\n",
    "                                      W_L,\n",
    "                                      last_10_games[i+6],\n",
    "                                      record,\n",
    "                                      place,\n",
    "                                      GB])\n",
    "    home[\"last_10_game\"] = last_10_game_list\n",
    "\n",
    "    ## head_to_head\n",
    "    try:\n",
    "        texts = x[0][2]\n",
    "        texts = texts.replace('(', ' (')\n",
    "        texts = texts.replace(\",\", \"\")\n",
    "        texts_split = texts.split(\" \")\n",
    "        head_to_head = texts_split.copy()\n",
    "        while \"\" in head_to_head:\n",
    "            head_to_head.remove(\"\")\n",
    "        for item in head_to_head:\n",
    "            if \"(\" in item:\n",
    "                head_to_head.remove(item)\n",
    "\n",
    "        head_to_head = head_to_head[head_to_head.index('head-to-head')+1:head_to_head.index('Season')]\n",
    "        a = head_to_head\n",
    "        new_head = []\n",
    "        i = 0\n",
    "        while i < len(head_to_head):\n",
    "            if head_to_head[i] in week:\n",
    "                j = i+1\n",
    "                while not 'W:' in head_to_head[j]:\n",
    "                    j += 1\n",
    "                new_head.append(head_to_head[i+1:j])\n",
    "                i = j+1\n",
    "            else:\n",
    "                i += 1\n",
    "        head_to_head = []\n",
    "        for col in new_head:\n",
    "            day = col[2] + date[col[0]] + col[1]\n",
    "            if '@' in col[5]:\n",
    "                flag = 0\n",
    "                loss = col[5].replace('@', '')\n",
    "            else:\n",
    "                flag = 1\n",
    "                loss = col[5]\n",
    "            win = col[3]\n",
    "\n",
    "            head_to_head.append([day, win, loss, flag, int(col[4]), int(col[6])])\n",
    "    except:\n",
    "        head_to_head = []\n",
    "        for i in range(10):\n",
    "            head_to_head.append([None, None, None, None, None, None])\n",
    "        \n",
    "    return road, home, head_to_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "years = ['2016']\n",
    "for year in years:\n",
    "    soup = BeautifulSoup(urlopen('http://www.baseball-reference.com/previews/' + year + '/'))\n",
    "    paragraphs = soup.find_all('a')\n",
    "    for i in range(len(paragraphs)):\n",
    "        if not ('-' in paragraphs[i].text or '/' in paragraphs[i].text):\n",
    "            break\n",
    "    if 'ALS' in paragraphs[i].text:\n",
    "        i += 1\n",
    "    paragraphs = paragraphs[i:]\n",
    "    data_times = list(map(lambda x: x.text, paragraphs))\n",
    "    data_times = list(map(lambda x: x[:-6], data_times))\n",
    "    for data_time in data_times:\n",
    "        if not year in data_time:\n",
    "            continue\n",
    "        print(year, data_time)\n",
    "        road, home, head_to_head = GetData(year, data_time)\n",
    "        data.append([data_time, road, home, head_to_head])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[\n",
    "    [\"0528\", 0, \"LAA\", 1, \"4-2\", \"21-29\", 4, 8.0],\n",
    "    [\"0527\", 0, \"LAA\", 0, \"2-7\", \"20-29\", 4, 9.0], ...\n",
    "]\n",
    "\n",
    "# @ -> 0, x -> 1\n",
    "# W -> 1, L -> 0\n",
    "# place: int\n",
    "# GB : float\n",
    "\n",
    "[\n",
    "    [\"20160528\", \"HOU\", \"LAA\", 0, 4, 2],\n",
    "    [\"20160527\", \"LAA\", \"HOU\", 1, 7, 2], ...\n",
    "]\n",
    "\n",
    "# @ -> 0, x -> 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listA = [\n",
    "    [\"dateA\", \"dictRoadA\", \"dictHomeA\", \"\"], [\"dateB\", \"dictRoadB\", \"dictHomeB\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraper = BRScraper()\n",
    "data = scraper.parse_tables(\"teams/BOS/2011.shtml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BRScraper:\n",
    "    \n",
    "    def __init__(self, server_url=\"http://www.baseball-reference.com/\"):\n",
    "        self.server_url = server_url\n",
    "    \n",
    "    def parse_tables(self, resource, table_ids=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Given a resource on the baseball-reference server (should consist of \n",
    "        the url after the hostname and slash), returns a dictionary keyed on \n",
    "        table id containing arrays of data dictionaries keyed on the header \n",
    "        columns. table_ids is a string or array of strings that can optionally \n",
    "        be used to filter out which stats tables to return. \n",
    "        \"\"\"\n",
    "\n",
    "        def is_parseable_table(tag):\n",
    "            if not tag.has_key(\"class\"): return False\n",
    "            return tag.name == \"table\" and \"stats_table\" in tag[\"class\"] and \"sortable\" in tag[\"class\"]\n",
    "\n",
    "        def is_parseable_row(tag):\n",
    "            if not tag.name == \"tr\": return False\n",
    "            if not tag.has_key(\"class\"): return True  # permissive\n",
    "            return \"league_average_table\" not in tag[\"class\"] and \"stat_total\" not in tag[\"class\"]\n",
    "\n",
    "        if isinstance(table_ids, str): table_ids = [table_ids]\n",
    "\n",
    "        soup = BeautifulSoup(urlopen(self.server_url + resource))\n",
    "        tables = soup.find_all(is_parseable_table)\n",
    "        data = {}\n",
    "\n",
    "        # Read through each table, read headers as dictionary keys\n",
    "        for table in tables:\n",
    "            \n",
    "            if table_ids != None and table[\"id\"] not in table_ids: continue\n",
    "            if verbose: print(\"Processing table \" + table[\"id\"])\n",
    "            data[table[\"id\"]] = []\n",
    "            \n",
    "            headers = table.find(\"thead\").find_all(\"th\")\n",
    "            header_names = []\n",
    "            for header in headers:\n",
    "                if header.string.strip() == 'Rk': continue\n",
    "                if header.string == None: \n",
    "                    base_header_name = u\"\"\n",
    "                else: base_header_name = header.string.strip()\n",
    "                if base_header_name in header_names:\n",
    "                    i = 1\n",
    "                    header_name = base_header_name + \"_\" + str(i)\n",
    "                    while header_name in header_names:\n",
    "                        i += 1\n",
    "                        header_name = base_header_name + \"_\" + str(i)\n",
    "                    if verbose: \n",
    "                        if base_header_name == \"\":\n",
    "                            print(\"Empty header relabeled as %s\" % header_name)\n",
    "                        else:\n",
    "                            print(\"Header %s relabeled as %s\" % (base_header_name, header_name))\n",
    "                else:\n",
    "                    header_name = base_header_name\n",
    "                header_names.append(header_name)\n",
    "            \n",
    "            rows = table.find(\"tbody\").find_all(is_parseable_row)\n",
    "            for row in rows:\n",
    "                entries = row.find_all(\"td\")\n",
    "                entry_data = []\n",
    "                for entry in entries:\n",
    "                    if entry.string == None:\n",
    "                        entry_data.append(entry.get_text())\n",
    "                    else:\n",
    "                        entry_data.append(entry.string.strip())\n",
    "                if len(entry_data) > 0:\n",
    "                    data[table[\"id\"]].append(dict(zip(header_names, entry_data)))\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
